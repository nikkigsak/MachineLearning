{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZS4RxcOAa_z7"
   },
   "source": [
    "### 1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?\n",
    "\n",
    "Ridge and Lasso Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVeKhKoEbYrP"
   },
   "source": [
    "### 2. Describe the main differences between supervised and unsupervised learning.\n",
    "1. Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning that makes use of labeled datasets. These datasets are used to train or \"supervise\" algorithms so that they can accurately identify data or forecast outcomes. The goal of supervised learning is to predict Y (outcome variable) using labeled input data  The model may test its accuracy and learn over time by using labeled inputs and outputs. Supervised learning is used for classifcation and regression problems.\n",
    "\n",
    "2. Unsupervised Learning \n",
    "\n",
    "Unsupervised learning analyzes and clusters unlabeled data sets using machine learning methods. The main distinction is in unsupervised learning, the objective is not to predict an outcome variable (response variable Y). The goal of unsupervised learning is to discovers trends and patterns in the data.Clustering, association, and dimensionality reduction are the three main tasks that unsupervised learning models are utilized for.\n",
    "\n",
    "3. Main differences\n",
    "*  The data is the major distinction between the two models. Supervised learning employs labeled datasets, whereas unsupervised learning uses unlabeled datasets. \n",
    "*  The purpose of supervised learning models is to predict outcomes, whereas the goal of unsupervised learning models is to find patterns in large unlabeled datasets.\n",
    "* Unsupervised learning is essential for comprehending the variance and grouping structure of unlabeled data. Additionally, unsupervised learning can be utilized as a pre-processing step prior to supervised learning. \n",
    "* Unless there is human interaction to validate the results and discoveries of unsupervised learning, supervised learning models are more accurate.\n",
    "\n",
    "Sources\n",
    "1. Class Lecture\n",
    "2. https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning#:~:text=The%20main%20difference%20between%20supervised,unsupervised%20learning%20algorithm%20does%20not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLomb4W6baNk"
   },
   "source": [
    "### 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n",
    "\n",
    "Supervised learning is the most common approach employed by machine learning practitioners. This is because supervised learning is considered a more accurate method in comparison to unsupervised learning (without human intervention). As mentioned in the previous question, supervised learning utilizes labeled data (inputs and outputs) and predicts the outcome based on the training data fed into the model. Since supervised learning relies on a process of labeling in order to “understand” information, there is a clear outcome that the models know to predict since the algorithm is trained to predict the outputs from the input data.\n",
    "\n",
    "\n",
    "The secondary approach, unsupervised learning, can be used to deal with large amounts of unlabeled data to discover patterns and trends. Since unsupervised learning uses datasets without labels to make inferences, it is great for finding patterns when you don't know exactly what you're looking for. Unsupervised learning is often faster and less costly, since it does not require human intervention or the task of data labeling. This also means that it can be utilized to decipher problems or challenges that cannot typically be solved by humans.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnCE6Fl3blzw"
   },
   "source": [
    "### 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?\n",
    "\n",
    "1. K Means Clustering\n",
    "\n",
    "In K Means Clustering, the algorithm finds 'K' centroids and then assigns each data point to the cluster closest to it, keeping the centroids as tiny as possible. This basically means that the model is used to groupthe data into a certain amount of clusters that is manually specified beforehand. To obtain the optimal number of clusters, an elbow curve plot is used where we observe the point in which the slope of the curve flattens. The average of the data, or determining the centroid, is what the 'means' in K-means refers to. A centroid is a location that represents the cluster's center. Additionally, in K Means clustering, each data point is assigned to one of the clusters through reducing the in-cluster sum of squares. The model is trying to minimize cluster variation within same clusters and maximize variation between different clusters. \n",
    "\n",
    "\n",
    "2. Hierarchichal Clustering\n",
    "\n",
    "Unlike K Means clustering, we do not know how many clusters we want beforehand with Hierarchical clustering. We begin with all of the observations in our data and work our way up to a dendogram. Instead of using an elbow plot, the dendrogram dictates the optimal amount of clusters chosen. We can use Hierarchical clustering to create tree structures based on data similarities, as well as determine how clusters connect to one another and the euclidean distance between data points. Linkage methods are a type of measurement method that is used to determine the distance between two methods.\n",
    "\n",
    "\n",
    "\n",
    "3. Principal Component Analysis\n",
    "\n",
    "\n",
    "PCA (Principle Components Analysis) is a valuable tool that allows us to summarize large feature sets with only a few principal components through dimensionality reduction. The PCA method provides us with our optimum collection of features. It generates a set of principal components that are uncorrelated, low in number, and rank ordered by variance. The first component has higher variance than the second, the second has higher variance than the third, and so on. THis means that we are able to throw away the lower ranked components as they contain little signal or significance. Therefore, method creates a low-dimensional representation of a dataset by finding a series of linear combinations of variables with the most variance and are uncorrelated with each other. Additionally, PCA is also an extremely useful tool for data visualization \n",
    "\n",
    "\n",
    "4. Main differences:\n",
    "* K Means clustering and Hierarchichal clustering are used for market segmentation, document clustering, image segmentation, and image compression while Principal Component Analysis is used for dimensionality reduction and visualization. \n",
    "* K Means clustering involves a pre-specified/chosen amount of clusters dictated by the elbow plot curve while Hierarchical clustering involves choosing an optimal amount of clusters from a dendogram. The number of clusters for Hierarchical clustering is also not chosen or known in advance, unlike K Means clustering.\n",
    "* PCA seeks a low-dimensional representation of the data that can explain a significant portion of the variance. Clustering examines the observations for homogenous groupings.\n",
    "* Instead of grouping the data, dinding associations within data is the ultimate goal of PCA.\n",
    "* Clustering is reducing the amount of data points by grouping them together while PCA reduces the amount of features while keeping the dataset's variance. \n",
    "\n",
    "Sources\n",
    "1. Class Lecture\n",
    "2. https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\n",
    "3. https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec\n",
    "4. https://towardsdatascience.com/understanding-pca-fae3e243731d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTKKTw8Jbnda"
   },
   "source": [
    "### 5.  What are the main benefits of using Principal Components Analysis?\n",
    "\n",
    "1. Removes Correlated Features and Reduces Overfitting\n",
    "\n",
    "There are numerous benefits to using Principal Components Analysis. The first benefit is that each principal component is independent of one another meaning there is no multicollineairty between the features. Using PCA can remove correlated variables that don't contribute to the model, resulting in a more efficient procedure. \n",
    "\n",
    "2. Reduces Overfitting\n",
    "\n",
    "When there are too many variables in a dataset, overfitting occurs. Since the number of features are effectively reduced through PCA, the method minimizes the overfitting of the data that is being dealth with. \n",
    "\n",
    "3. Improves Visualization \n",
    "\n",
    "PCA transforms a high dimensional data into 2 dimensional data (low dimension) which makes visualization significantly easier.\n",
    "\n",
    "4. Improve/Speed Up Algorithm Performance\n",
    "\n",
    "PCA is a popular method for speeding up your Machine Learning algorithm by removing associated variables that don't help to decision making. With fewer features, the training time of the algorithms is greatly reduced.\n",
    "\n",
    "5. High Variance\n",
    "\n",
    "The model employs maximum variation, which means that lesser variations are ignored, resulting in a reduction of noise in the model.\n",
    "\n",
    "Source \n",
    "1. Class Lecture\n",
    "2. http://theprofessionalspoint.blogspot.com/2019/03/advantages-and-disadvantages-of_4.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PZfs7mfbsKY"
   },
   "source": [
    "### 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.\n",
    "\n",
    "Although Multiplay Perceptron (MLP) was once employed in computer vision, Convolution Neural Networks (CNN) are far better suited to the task since they consider the dimensional information of a picture, which MLPs do not. Therefore, MLP is no longer considered adequate for advanced computer vision tasks.This is because MLPs have fully connected layers, which means that each perceptron is connected to every other perceptron. The drawback is that the overall amount of parameters might quickly grow to be high and inefficient due to the redundancy in such high dimensions. With CNNs, Rather than being entirely connected, layers are sparsely connected. Every node is not connected to every other node in the network. Additionally, another key difference is that MLPs disregard spatial information and takes flattened vectors of inputs while CNNs accepts both matrices and vectors as inputs. Finally, CNNS can take into account local connectivity, which means that each filter is panned across the entire image based on its size and stride. The weights are smaller and shared, resulting in less waste and making training easier. In comparison to MLPs, it is a more effective approach that can also go deeper.\n",
    "\n",
    "Sources\n",
    "1. Class lecture\n",
    "2. https://www.peculiar-coding-endeavours.com/2019/mlp_vs_cnn/\n",
    "3. https://medium.com/data-science-bootcamp/multilayer-perceptron-mlp-vs-convolutional-neural-network-in-deep-learning-c890f487a8f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Tmnj6XubvqS"
   },
   "source": [
    "### 7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4kNd0Q5eNPL"
   },
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "## tf.keras\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyEQW4zHgQXS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "## 3 hidden layers activate all with relu\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(units=50, activation='relu', input_dim=?random_input_dim)) ## 50 hidden units\n",
    "model.add(Dense(units=100, activation='relu')) ## 100 hidden units\n",
    "model.add(Dense(units=150, activation='relu')) ## 150 hidden units \n",
    "model.add(Dense(units=5, activation='softmax')) ## output layer softmax classify 5 categories\n",
    "model.compile(optimizer='sgd', ## stochastic gradient descent \n",
    "              loss='categorical_crossentropy', ## multiple categories \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqTqPQtynjHE"
   },
   "source": [
    "### 8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-8QW6X-ngNm"
   },
   "outputs": [],
   "source": [
    "## 2 hidden layers activate all with relu \n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(units=75, activation='relu', input_dim=?random_input_dim)) ## 75 hidden units\n",
    "model.add(Dense(units=150, activation='relu')) ## 150 hidden units\n",
    "model.add(Dense(units=1, activation='sigmoid')) ## sigmoid 2 categories, binary dependent variable \n",
    "model.compile(optimizer='sgd', ## stochastic gradient descent \n",
    "              loss='binary_crossentropy', ## binary categories \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rWT6sADpc_f"
   },
   "source": [
    "### 9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-8XU0Yhpefy"
   },
   "outputs": [],
   "source": [
    "## import libraries \n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "def build_model():\n",
    "  \"\"\"\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), padding='valid', \n",
    "                 input_shape=shape_ord)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(28),(3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(10)) ## classify 10 categories \n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', ## classify multiple categories \n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_A-uK0ssT1U"
   },
   "source": [
    "### 10.  Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpJFNmX4sTb6"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  \"\"\"\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', \n",
    "                 input_shape=shape_ord)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "model.add(Dense(128)) ## 128 hidden units fully connected layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(128)) \n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6)) ## classify 6 categories \n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', ## classify multiple categories \n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE9n9nwA11y9"
   },
   "source": [
    "### Bonus Question:\n",
    "Username: nikkigsak"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SG4011 Final exam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
